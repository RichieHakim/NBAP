{
 "cells": [
  {
   "cell_type": "raw",
   "id": "775bba49",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "\"\"\"  High level interface to the CASCADE package\n",
    "\n",
    "This file contains functions to train networks for spike prediction ('train_model')\n",
    "and to use existing networks to predict spiking activity ('predict').\n",
    "\n",
    "\n",
    "A typical workflow for applying an existing network to calcium imaging data,\n",
    "shown in the \"demo_predict.py\" script:\n",
    "\n",
    "  1)  Load calcium imaging data as a dF/F matrix\n",
    "  2)  Load a predefined model; the model should match the properties of the calcium\n",
    "      imaging dataset (frame rate, noise levels, ground truth datasets)\n",
    "  3)  Use the model and the dF/F matrix as inputs for the function 'predict'\n",
    "  4)  Predictions will be saved. Done!\n",
    "\n",
    "A typical workflow for training a new network would be the following,\n",
    "shown in the \"demo_train.py\" script:\n",
    "\n",
    "  1)  Define a model (frame rate, noise levels, ground truth datasets; additional parameters)\n",
    "  2)  Use the model as input to the function 'train_model'\n",
    "  3)  The trained models will be saved together with a configuration file (YAML). Done!\n",
    "\n",
    "\n",
    "Additional functions in this file are used to navigate different models ('get_model_paths', 'create_model_folder',  'verify_config_dict').\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "from . import config, utils\n",
    "\n",
    "def hi():\n",
    "    print(\"hi\")\n",
    "\n",
    "def train_model(\n",
    "    model_name, model_folder=\"Pretrained_models\", ground_truth_folder=\"Ground_truth\"\n",
    "):\n",
    "\n",
    "    \"\"\"Train neural network with parameters specified in the config.yaml file in the model folder\n",
    "\n",
    "    In this function, a model is configured (defined in the input 'model_name': frame rate, noise levels, ground truth datasets, etc.).\n",
    "    The ground truth is resampled (function 'preprocess_groundtruth_artificial_noise_balanced', defined in \"utils.py\").\n",
    "    The network architecture is defined (function 'define_model', defined in \"utils.py\").\n",
    "    The thereby defined model is trained with the resampled ground truth data.\n",
    "    The trained model with its weight and configuration details is saved to disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Name of the model, e.g. 'Universal_30Hz_smoothing100ms'\n",
    "        This name has to correspond to the folder with the config.yaml file which defines the model parameters\n",
    "\n",
    "    model_folder: str\n",
    "        Absolute or relative path, which defines the location of the specified model_name folder\n",
    "        Default value 'Pretrained_models' assumes a current working directory in the Cascade folder\n",
    "\n",
    "    ground_truth_folder : str\n",
    "        Absolute or relative path, which defines the location of the ground truth datasets\n",
    "        Default value 'Ground_truth'  assumes a current working directory in the Cascade folder\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    None\n",
    "        All results are saved in the folder model_name as .h5 files containing the trained model\n",
    "\n",
    "    \"\"\"\n",
    "    import tensorflow.keras\n",
    "    from tensorflow.keras.optimizers import Adagrad\n",
    "\n",
    "    model_path = os.path.join(model_folder, model_name)\n",
    "    cfg_file = os.path.join(model_path, \"config.yaml\")\n",
    "\n",
    "    # check if configuration file can be found\n",
    "    if not os.path.isfile(cfg_file):\n",
    "        m = (\n",
    "            'The configuration file \"config.yaml\" can not be found at the location \"{}\".\\n'.format(\n",
    "                os.path.abspath(cfg_file)\n",
    "            )\n",
    "            + 'You have provided the model \"{}\" at the absolute or relative path \"{}\".\\n'.format(\n",
    "                model_name, model_folder\n",
    "            )\n",
    "            + 'Please check if there is a folder for model \"{}\" at the location \"{}\".'.format(\n",
    "                model_name, os.path.abspath(model_folder)\n",
    "            )\n",
    "        )\n",
    "        print(m)\n",
    "        raise Exception(m)\n",
    "\n",
    "    # load cfg dictionary from config.yaml file\n",
    "    cfg = config.read_config(cfg_file)\n",
    "    verbose = cfg[\"verbose\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Used configuration for model fitting (file {}):\\n\".format(\n",
    "                os.path.abspath(cfg_file)\n",
    "            )\n",
    "        )\n",
    "        for key in cfg:\n",
    "            print(\"{}:\\t{}\".format(key, cfg[key]))\n",
    "\n",
    "        print(\"\\n\\nModels will be saved into this folder:\", os.path.abspath(model_path))\n",
    "\n",
    "    # add base folder to selected training datasets\n",
    "    training_folders = [\n",
    "        os.path.join(ground_truth_folder, ds) for ds in cfg[\"training_datasets\"]\n",
    "    ]\n",
    "\n",
    "    # check if the training datasets can be found\n",
    "    missing = False\n",
    "    for folder in training_folders:\n",
    "        if not os.path.isdir(folder):\n",
    "            print(\n",
    "                'The folder \"{}\" could not be found at the specified location \"{}\"'.format(\n",
    "                    folder, os.path.abspath(folder)\n",
    "                )\n",
    "            )\n",
    "            missing = True\n",
    "    if missing:\n",
    "        m = (\n",
    "            'At least one training dataset could not be located.\\nThis could mean that the given path \"{}\" '.format(\n",
    "                ground_truth_folder\n",
    "            )\n",
    "            + \"does not specify the correct location or that e.g. a training dataset referenced in the config.yaml file \"\n",
    "            + \"contained a typo.\"\n",
    "        )\n",
    "        print(m)\n",
    "        raise Exception(m)\n",
    "\n",
    "    start = time.time()\n",
    "    # Update model fitting status\n",
    "    cfg[\"training_finished\"] = \"Running\"\n",
    "    config.write_config(cfg, os.path.join(model_path, \"config.yaml\"))\n",
    "\n",
    "    nr_model_fits = len(cfg[\"noise_levels\"]) * cfg[\"ensemble_size\"]\n",
    "    print(\"Fitting a total of {} models:\".format(nr_model_fits))\n",
    "\n",
    "    curr_model_nr = 0\n",
    "\n",
    "    print(training_folders[0])\n",
    "\n",
    "    for noise_level in cfg[\"noise_levels\"]:\n",
    "        for ensemble in range(cfg[\"ensemble_size\"]):\n",
    "            # train 'ensemble_size' (e.g. 5) models for each noise level\n",
    "\n",
    "            curr_model_nr += 1\n",
    "            print(\n",
    "                \"\\nFitting model {} with noise level {} (total {} out of {}).\".format(\n",
    "                    ensemble + 1, noise_level, curr_model_nr, nr_model_fits\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if cfg[\"sampling_rate\"] > 30:\n",
    "\n",
    "                cfg[\"windowsize\"] = int(np.power(cfg[\"sampling_rate\"] / 30, 0.25) * 64)\n",
    "                # write again the config file to update the adjusted window size value\n",
    "                config.write_config(cfg, os.path.join(model_path, \"config.yaml\"))\n",
    "\n",
    "                print(\n",
    "                    \"Window size enlarged to \"\n",
    "                    + str(cfg[\"windowsize\"])\n",
    "                    + \" time points due to the high calcium imaging sampling rate(\"\n",
    "                    + str(cfg[\"sampling_rate\"])\n",
    "                    + \").\"\n",
    "                )\n",
    "\n",
    "            # preprocess dataset to get uniform dataset for training\n",
    "            X, Y = utils.preprocess_groundtruth_artificial_noise_balanced(\n",
    "                ground_truth_folders=training_folders,\n",
    "                before_frac=cfg[\"before_frac\"],\n",
    "                windowsize=cfg[\"windowsize\"],\n",
    "                after_frac=1 - cfg[\"before_frac\"],\n",
    "                noise_level=noise_level,\n",
    "                sampling_rate=cfg[\"sampling_rate\"],\n",
    "                smoothing=cfg[\"smoothing\"] * cfg[\"sampling_rate\"],\n",
    "                omission_list=[],\n",
    "                permute=1,\n",
    "                verbose=cfg[\"verbose\"],\n",
    "                replicas=1,\n",
    "                causal_kernel=cfg[\"causal_kernel\"],\n",
    "            )\n",
    "\n",
    "            model = utils.define_model(\n",
    "                filter_sizes=cfg[\"filter_sizes\"],\n",
    "                filter_numbers=cfg[\"filter_numbers\"],\n",
    "                dense_expansion=cfg[\"dense_expansion\"],\n",
    "                windowsize=cfg[\"windowsize\"],\n",
    "                loss_function=cfg[\"loss_function\"],\n",
    "                optimizer=cfg[\"optimizer\"],\n",
    "            )\n",
    "\n",
    "            optimizer = Adagrad(learning_rate=0.05)\n",
    "            model.compile(loss=cfg[\"loss_function\"], optimizer=optimizer)\n",
    "\n",
    "            cfg[\"nr_of_epochs\"] = np.minimum(\n",
    "                cfg[\"nr_of_epochs\"], np.int(10 * np.floor(5e6 / len(X)))\n",
    "            )\n",
    "\n",
    "            model.fit(\n",
    "                X,\n",
    "                Y,\n",
    "                batch_size=cfg[\"batch_size\"],\n",
    "                epochs=cfg[\"nr_of_epochs\"],\n",
    "                verbose=cfg[\"verbose\"],\n",
    "            )\n",
    "\n",
    "            # save model\n",
    "            file_name = \"Model_NoiseLevel_{}_Ensemble_{}.h5\".format(\n",
    "                int(noise_level), ensemble\n",
    "            )\n",
    "            model.save(os.path.join(model_path, file_name))\n",
    "            print(\"Saved model:\", file_name)\n",
    "\n",
    "    # Update model fitting status\n",
    "    # cfg['training_finished'] = 'Yes'\n",
    "    # config.write_config(cfg, os.path.join( model_path, 'config.yaml' ))\n",
    "\n",
    "    print(\"\\n\\nDone!\")\n",
    "    print(\"Runtime: {:.0f} min\".format((time.time() - start) / 60))\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model_name, traces, model_folder=\"Pretrained_models\", threshold=0, padding=np.nan\n",
    "):\n",
    "\n",
    "    \"\"\"Use a specific trained neural network ('model_name') to predict spiking activity for calcium traces ('traces')\n",
    "\n",
    "    In this function, a already trained model (generated by 'train_model' or downloaded) is loaded.\n",
    "    The model (frame rate, noise levels, ground truth datasets) should be chosen\n",
    "      to match the properties of the calcium recordings in 'traces'.\n",
    "    An ensemble of 5 models is loaded for each noise level.\n",
    "    These models are used to predict spiking activity of neurons from 'traces' with the same noise levels.\n",
    "    The predictions are made in the line with 'model.predict()'.\n",
    "    The predictions are returned as a matrix 'Y_predict'.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    model_name : str\n",
    "        Name of the model, e.g. 'Universal_30Hz_smoothing100ms'\n",
    "        This name has to correspond to the folder in which the config.yaml and .h5 files are stored which define\n",
    "        the trained model\n",
    "\n",
    "    traces : 2d numpy array (neurons x nr_timepoints)\n",
    "        Df/f traces with recorded fluorescence (as fractions, not in percents) on which the spiking activity will\n",
    "        be predicted. Required shape: (neurons x nr_timepoints)\n",
    "\n",
    "    model_folder: str\n",
    "        Absolute or relative path, which defines the location of the specified model_name folder\n",
    "        Default value 'Pretrained_models' assumes a current working directory in the Cascade folder\n",
    "\n",
    "    threshold : int or boolean\n",
    "        Allowed values: 0, 1 or False\n",
    "            0: All negative values are set to 0\n",
    "            1 or True: Threshold signal to set every signal which is smaller than the expected signal size\n",
    "                       of an action potential to zero (with dilated mask)\n",
    "            False: No thresholding. The result can contain negative values as well\n",
    "\n",
    "    padding : 0 or np.nan\n",
    "        Value which is inserted for datapoints, where no prediction can be made (because of window around timepoint of prediction)\n",
    "        Default value: np.nan, another recommended value would be 0 which circumvents some problems with following analysis.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    predicted_activity: 2d numpy array (neurons x nr_timepoints)\n",
    "        Spiking activity as predicted by the model. The shape is the same as 'traces'\n",
    "        This array can contain NaNs if the value 'padding' was np.nan as input argument\n",
    "\n",
    "    \"\"\"\n",
    "    import tensorflow.keras\n",
    "    from tensorflow.keras.models import load_model\n",
    "\n",
    "    model_path = os.path.join(model_folder, model_name)\n",
    "    cfg_file = os.path.join(model_path, \"config.yaml\")\n",
    "\n",
    "    # check if configuration file can be found\n",
    "    if not os.path.isfile(cfg_file):\n",
    "        m = (\n",
    "            'The configuration file \"config.yaml\" can not be found at the location \"{}\".\\n'.format(\n",
    "                os.path.abspath(cfg_file)\n",
    "            )\n",
    "            + 'You have provided the model \"{}\" at the absolute or relative path \"{}\".\\n'.format(\n",
    "                model_name, model_folder\n",
    "            )\n",
    "            + 'Please check if there is a folder for model \"{}\" at the location \"{}\".'.format(\n",
    "                model_name, os.path.abspath(model_folder)\n",
    "            )\n",
    "        )\n",
    "        print(m)\n",
    "        raise Exception(m)\n",
    "\n",
    "    # Load config file\n",
    "    cfg = config.read_config(cfg_file)\n",
    "\n",
    "    # extract values from config file into variables\n",
    "    verbose = cfg[\"verbose\"]\n",
    "    training_data = cfg[\"training_datasets\"]\n",
    "    ensemble_size = cfg[\"ensemble_size\"]\n",
    "    batch_size = cfg[\"batch_size\"]\n",
    "    sampling_rate = cfg[\"sampling_rate\"]\n",
    "    before_frac = cfg[\"before_frac\"]\n",
    "    window_size = cfg[\"windowsize\"]\n",
    "    noise_levels_model = cfg[\"noise_levels\"]\n",
    "    smoothing = cfg[\"smoothing\"]\n",
    "    causal_kernel = cfg[\"causal_kernel\"]\n",
    "\n",
    "    model_description = (\n",
    "        \"\\n \\nThe selected model was trained on \"\n",
    "        + str(len(training_data))\n",
    "        + \" datasets, with \"\n",
    "        + str(ensemble_size)\n",
    "        + \" ensembles for each noise level, at a sampling rate of \"\n",
    "        + str(sampling_rate)\n",
    "        + \"Hz,\"\n",
    "    )\n",
    "    if causal_kernel:\n",
    "        model_description += (\n",
    "            \" with a resampled ground truth that was smoothed with a causal kernel\"\n",
    "        )\n",
    "    else:\n",
    "        model_description += (\n",
    "            \" with a resampled ground truth that was smoothed with a Gaussian kernel\"\n",
    "        )\n",
    "    model_description += (\n",
    "        \" of a standard deviation of \"\n",
    "        + str(int(1000 * smoothing))\n",
    "        + \" milliseconds. \\n \\n\"\n",
    "    )\n",
    "    print(model_description)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Loaded model was trained at frame rate {} Hz\".format(sampling_rate))\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Given argument traces contains {} neurons and {} frames.\".format(\n",
    "                traces.shape[0], traces.shape[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # calculate noise levels for each trace\n",
    "    trace_noise_levels = utils.calculate_noise_levels(traces, sampling_rate)\n",
    "\n",
    "    print(\n",
    "        \"Noise levels (mean, std; in standard units): \"\n",
    "        + str(int(np.nanmean(trace_noise_levels * 100)) / 100)\n",
    "        + \", \"\n",
    "        + str(int(np.nanstd(trace_noise_levels * 100)) / 100)\n",
    "    )\n",
    "\n",
    "    # Get model paths as dictionary (key: noise_level) with lists of model\n",
    "    # paths for the different ensembles\n",
    "    model_dict = get_model_paths(model_path)  # function defined below\n",
    "    if verbose > 2:\n",
    "        print(\"Loaded models:\", str(model_dict))\n",
    "\n",
    "    # XX has shape: (neurons, timepoints, windowsize)\n",
    "    XX = utils.preprocess_traces(\n",
    "        traces, before_frac=before_frac, window_size=window_size\n",
    "    )\n",
    "    Y_predict = np.zeros((XX.shape[0], XX.shape[1]))\n",
    "\n",
    "    # Use for each noise level the matching model\n",
    "    for i, model_noise in enumerate(noise_levels_model):\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nPredictions for noise level {}:\".format(model_noise))\n",
    "\n",
    "        # select neurons which have this noise level:\n",
    "        if i == 0:  # lowest noise\n",
    "            neuron_idx = np.where(trace_noise_levels < model_noise + 0.5)[0]\n",
    "        elif i == len(noise_levels_model) - 1:  # highest noise\n",
    "            neuron_idx = np.where(trace_noise_levels >= model_noise - 0.5)[0]\n",
    "        else:\n",
    "            neuron_idx = np.where(\n",
    "                (trace_noise_levels >= model_noise - 0.5)\n",
    "                & (trace_noise_levels < model_noise + 0.5)\n",
    "            )[0]\n",
    "\n",
    "        if len(neuron_idx) == 0:  # no neurons were selected\n",
    "            if verbose:\n",
    "                print(\"\\tNo neurons for this noise level\")\n",
    "            continue  # jump to next noise level\n",
    "\n",
    "        # load keras models for the given noise level\n",
    "        models = list()\n",
    "        for model_path in model_dict[model_noise]:\n",
    "            models.append(load_model(model_path))\n",
    "\n",
    "        # select neurons and merge neurons and timepoints into one dimension\n",
    "        XX_sel = XX[neuron_idx, :, :]\n",
    "\n",
    "        XX_sel = np.reshape(\n",
    "            XX_sel, (XX_sel.shape[0] * XX_sel.shape[1], XX_sel.shape[2])\n",
    "        )\n",
    "        XX_sel = np.expand_dims(\n",
    "            XX_sel, axis=2\n",
    "        )  # add empty third dimension to match training shape\n",
    "\n",
    "        for j, model in enumerate(models):\n",
    "            if verbose:\n",
    "                print(\"\\t... ensemble\", j)\n",
    "\n",
    "            prediction_flat = model.predict(XX_sel, batch_size, verbose=verbose)\n",
    "            prediction = np.reshape(prediction_flat, (len(neuron_idx), XX.shape[1]))\n",
    "\n",
    "            Y_predict[neuron_idx, :] += prediction / len(models)  # average predictions\n",
    "\n",
    "        # remove models from memory\n",
    "        tensorflow.keras.backend.clear_session()\n",
    "\n",
    "    if threshold is False:  # only if 'False' is passed as argument\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Skipping the thresholding. There can be negative values in the result.\"\n",
    "            )\n",
    "\n",
    "    elif threshold == 1:  # (1 or True)\n",
    "        # Cut off noise floor (lower than 1/e of a single action potential)\n",
    "\n",
    "        from scipy.ndimage.filters import gaussian_filter\n",
    "        from scipy.ndimage.morphology import binary_dilation\n",
    "\n",
    "        # find out empirically  how large a single AP is (depends on frame rate and smoothing)\n",
    "        single_spike = np.zeros(\n",
    "            1001,\n",
    "        )\n",
    "        single_spike[501] = 1\n",
    "        single_spike_smoothed = gaussian_filter(\n",
    "            single_spike.astype(float), sigma=smoothing * sampling_rate\n",
    "        )\n",
    "        threshold_value = np.max(single_spike_smoothed) / np.exp(1)\n",
    "\n",
    "        # Set everything below threshold to zero.\n",
    "        # Use binary dilation to avoid clipping of true events.\n",
    "        for neuron in range(Y_predict.shape[0]):\n",
    "            # ignore warning because of nan's in Y_predict in comparison with value\n",
    "            with np.errstate(invalid=\"ignore\"):\n",
    "                activity_mask = Y_predict[neuron, :] > threshold_value\n",
    "            activity_mask = binary_dilation(\n",
    "                activity_mask, iterations=int(smoothing * sampling_rate)\n",
    "            )\n",
    "\n",
    "            Y_predict[neuron, ~activity_mask] = 0\n",
    "\n",
    "            Y_predict[\n",
    "                Y_predict < 0\n",
    "            ] = 0  # set possible negative values in dilated mask to 0\n",
    "\n",
    "    elif threshold == 0:\n",
    "        # ignore warning because of nan's in Y_predict in comparison with value\n",
    "        with np.errstate(invalid=\"ignore\"):\n",
    "            Y_predict[Y_predict < 0] = 0\n",
    "\n",
    "    else:\n",
    "        raise Exception(\n",
    "            'Invalid value of threshold \"{}\". Only 0, 1 (or True) or False allowed'.format(\n",
    "                threshold\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # NaN or 0 for first and last datapoints, for which no predictions can be made\n",
    "    Y_predict[:, 0 : int(before_frac * window_size)] = padding\n",
    "    Y_predict[:, -int((1 - before_frac) * window_size) :] = padding\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "    return Y_predict\n",
    "\n",
    "\n",
    "def verify_config_dict(config_dictionary):\n",
    "\n",
    "    \"\"\"Perform some test to catch the most likely errors when creating config files\"\"\"\n",
    "\n",
    "    # TODO: Implement\n",
    "    print(\"Not implemented yet...\")\n",
    "\n",
    "\n",
    "def create_model_folder(config_dictionary, model_folder=\"Pretrained_models\"):\n",
    "\n",
    "    \"\"\"Creates a new folder in model_folder and saves config.yaml file there\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config_dictionary : dict\n",
    "        Dictionary with keys like 'model_name' or 'training_datasets'\n",
    "        Values which are not specified will be set to default values defined in\n",
    "        the config_template in config.py\n",
    "\n",
    "    model_folder : str\n",
    "        Absolute or relative path, which defines the location at which the new\n",
    "        folder containing the config file will be created\n",
    "        Default value 'Pretrained_models' assumes a current working directory\n",
    "        in the Cascade folder\n",
    "\n",
    "    \"\"\"\n",
    "    cfg = config_dictionary  # shorter name\n",
    "\n",
    "    # TODO: call here verify_config_dict\n",
    "\n",
    "    # TODO: check here the current directory, might not be the main folder...\n",
    "    model_path = os.path.join(model_folder, cfg[\"model_name\"])\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        # create folder\n",
    "        try:\n",
    "            os.mkdir(model_path)\n",
    "            print('Created new directory \"{}\"'.format(os.path.abspath(model_path)))\n",
    "        except:\n",
    "            print(model_path + \" already exists\")\n",
    "\n",
    "        # save config file into the folder\n",
    "        config.write_config(cfg, os.path.join(model_path, \"config.yaml\"))\n",
    "\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            \"There is already a folder called {}. \".format(cfg[\"model_name\"])\n",
    "            + \"Please rename your model.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_model_paths(model_path):\n",
    "\n",
    "    \"\"\"Find all models in the model folder and return as dictionary\n",
    "    ( Helper function called by predict() )\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model_dict : dict\n",
    "        Dictionary with noise_level (int) as keys and entries are lists of model paths\n",
    "\n",
    "    \"\"\"\n",
    "    import glob, re\n",
    "\n",
    "    all_models = glob.glob(os.path.join(model_path, \"*.h5\"))\n",
    "    all_models = sorted(all_models)  # sort\n",
    "\n",
    "    # Exception in case no model was found to catch this mistake where it happened\n",
    "    if len(all_models) == 0:\n",
    "        m = 'No models (*.h5 files) were found in the specified folder \"{}\".'.format(\n",
    "            os.path.abspath(model_path)\n",
    "        )\n",
    "        raise Exception(m)\n",
    "\n",
    "    # dictionary with key for noise level, entries are lists of models\n",
    "    model_dict = dict()\n",
    "\n",
    "    for model_path in all_models:\n",
    "        try:\n",
    "            noise_level = int(re.findall(\"_NoiseLevel_(\\d+)\", model_path)[0])\n",
    "        except:\n",
    "            print(\"Error while processing the file with name: \", model_path)\n",
    "            raise\n",
    "\n",
    "        # add model path to the model dictionary\n",
    "        if noise_level not in model_dict:\n",
    "            model_dict[noise_level] = list()\n",
    "        model_dict[noise_level].append(model_path)\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def download_model(\n",
    "    model_name,\n",
    "    model_folder=\"Pretrained_models\",\n",
    "    info_file_link=\"https://drive.switch.ch/index.php/s/kjGm0qsZdofY629/download\",\n",
    "    verbose=1,\n",
    "):\n",
    "    \"\"\"Download and unzip pretrained model from the online repository\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Name of the model, e.g. 'Universal_30Hz_smoothing100ms'\n",
    "        This name has to correspond to a pretrained model that is available for download\n",
    "        To see available models, run this function with model_name='update_models' and\n",
    "        check the downloaded file 'available_models.yaml'\n",
    "\n",
    "    model_folder: str\n",
    "        Absolute or relative path, which defines the location of the specified model_name folder\n",
    "        Default value 'Pretrained_models' assumes a current working directory in the Cascade folder\n",
    "\n",
    "    info_file_link: str\n",
    "        Direct download link to yaml file which contains download links for new models.\n",
    "        Default value is official repository of models.\n",
    "\n",
    "    verbose : int\n",
    "        If 0, no messages are printed. if larger than 0, the user is informed about status.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    from urllib.request import urlopen\n",
    "    import zipfile\n",
    "\n",
    "    # Download the current yaml file with information about available models first\n",
    "    new_file = os.path.join(model_folder, \"available_models.yaml\")\n",
    "    with urlopen(info_file_link) as response:\n",
    "        text = response.read()\n",
    "\n",
    "    with open(new_file, \"wb\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    # check if the specified model_name is present\n",
    "    download_config = config.read_config(\n",
    "        new_file\n",
    "    )  # orderedDict with model names as keys\n",
    "\n",
    "    if model_name not in download_config.keys():\n",
    "        if model_name == \"update_models\":\n",
    "            print(\n",
    "                \"You can now check the updated available_models.yaml file for valid model names.\"\n",
    "            )\n",
    "            print(\"File location:\", os.path.abspath(new_file))\n",
    "            return\n",
    "\n",
    "        raise Exception(\n",
    "            'The specified model_name \"{}\" is not in the list of available models. '.format(\n",
    "                model_name\n",
    "            )\n",
    "            + \"Available models for download are: {}\".format(\n",
    "                list(download_config.keys())\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print('Downloading and extracting new model \"{}\"...'.format(model_name))\n",
    "\n",
    "    # download and save .zip file of model\n",
    "    download_link = download_config[model_name][\"Link\"]\n",
    "    with urlopen(download_link) as response:\n",
    "        data = response.read()\n",
    "\n",
    "    tmp_file = os.path.join(model_folder, \"tmp_zipped_model.zip\")\n",
    "    with open(tmp_file, \"wb\") as f:\n",
    "        f.write(data)\n",
    "\n",
    "    # unzip the model and save in the corresponding folder\n",
    "    with zipfile.ZipFile(tmp_file, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(path=os.path.join(model_folder, model_name))\n",
    "\n",
    "    os.remove(tmp_file)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            'Pretrained model was saved in folder \"{}\"'.format(\n",
    "                os.path.abspath(os.path.join(model_folder, model_name))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12250866",
   "metadata": {},
   "source": [
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"cascade2p\",\n",
    "    version=\"1.0\",\n",
    "    description=\"Calibrated inference of spiking from calcium ΔF/F data using deep networks\",\n",
    "    author=\"Peter Rupprecht\",\n",
    "    author_email=\"\",\n",
    "    packages=find_packages(),\n",
    "    python_requires=\">=3.6, <3.9\",\n",
    "    install_requires=[\n",
    "        \"numpy\",\n",
    "        \"scipy\",\n",
    "        \"matplotlib\",\n",
    "        \"tensorflow==2.3\",  # pip install CPU and GPU tensorflow\n",
    "        \"keras==2.3.1\",\n",
    "        \"h5py==2.10.0\",\n",
    "        \"seaborn\",\n",
    "        \"ruamel.yaml\",\n",
    "        \"spyder\",\n",
    "    ],\n",
    ")\n",
    "~       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b26f2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda Environment: Cascade\n"
     ]
    }
   ],
   "source": [
    "# widen jupyter notebook window\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "\n",
    "# check environment\n",
    "import os\n",
    "print(f'Conda Environment: ' + os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f1405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd72f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import scipy.interpolate\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956fd899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "dir_github = '/media/rich/Home_Linux_partition/github_repos/'\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/n/data1/hms/neurobio/sabatini/rich/github_repos/')\n",
    "sys.path.append(dir_github)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import ca2p_preprocessing, timeSeries, math_functions, indexing\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from NBAP import import_data, align_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8928eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tYAML reader installed (version 0.16.12).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 21:17:01.726460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKeras installed (version 2.4.0).\n",
      "\tTensorflow installed (version 2.4.1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rich/OS/Users/Richard/Linux_stuff_on_OS/conda_envs/envs/Cascade/lib/python3.7/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import cascade2p\n",
    "from cascade2p import checks\n",
    "checks.check_packages()\n",
    "from cascade2p import cascade # local folder\n",
    "from cascade2p.utils import plot_dFF_traces, plot_noise_level_distribution, plot_noise_matched_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cace698",
   "metadata": {},
   "source": [
    "import torchinterp1d"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90a8fa72",
   "metadata": {},
   "source": [
    "import all the s2p outputs\n",
    "preprocess them (cascade)\n",
    "quality check them\n",
    "\n",
    "import all the wavesurfer files\n",
    "get ws_times for all frames (assume you can trust first and last edges of pulses)\n",
    "get ws_times of all experiment events\n",
    "\n",
    "import experiment file\n",
    "get event types\n",
    "save metadata on event durations and types etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4eba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# dir_S2p = Path(r'/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210409/baseline/suite2p/plane0/')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5b15ce8",
   "metadata": {},
   "source": [
    "dir_batch = ['/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210409/baseline/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210410/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210411/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210412/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210413/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210414/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210415/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210416/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210417/exp/suite2p/plane0/',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e02257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_batch = ['/media/rich/bigSSD/res2p/scanimage data/round 5 experiments/mouse 2_6/20210417/suite2p/plane0',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210410/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210411/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210412/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210413/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210414/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210415/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210416/exp/suite2p/plane0/',\n",
    "#              '/n/data1/hms/neurobio/sabatini/rich/data/res2p/round_5_experiments/mouse_2_6/scanimage/20210417/exp/suite2p/plane0/',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a4d94a1",
   "metadata": {},
   "source": [
    "import pickle\n",
    "import json\n",
    "with open('/n/data1/hms/neurobio/sabatini/rich/analysis/mouse_2_6/neural_data/spike_prob_dayNames.json', 'w') as f:\n",
    "    json.dump(dir_batch, f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "229157e8",
   "metadata": {},
   "source": [
    "with open('/n/data1/hms/neurobio/sabatini/rich/analysis/mouse_2_6/neural_data/spike_prob_dayNames.json', 'r') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c237c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated dFoF. Total elapsed time: 5.88 seconds\n",
      "ThreadPool elapsed time : 0.43 s. Now unpacking list into array.\n",
      "Calculated convolution. Total elapsed time: 1.37 seconds\n",
      "The configuration file \"config.yaml\" can not be found at the location \"/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models/Global_EXC_30Hz_smoothing50ms_causalkernel/config.yaml\".\n",
      "You have provided the model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the absolute or relative path \"/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models\".\n",
      "Please check if there is a folder for model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the location \"/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models\".\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The configuration file \"config.yaml\" can not be found at the location \"/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models/Global_EXC_30Hz_smoothing50ms_causalkernel/config.yaml\".\nYou have provided the model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the absolute or relative path \"/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models\".\nPlease check if there is a folder for model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the location \"/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9957/3245243456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                  \u001b[0mmodel_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                  \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                                 ) for batch in indexing.make_batches(dFoF_smooth, num_batches=20)], axis=0)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     np.save(f'/n/data1/hms/neurobio/sabatini/rich/spike_prob_{iter_day}.npy', np.single(spike_prob))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9957/3245243456.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                                  \u001b[0mmodel_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                  \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                                 ) for batch in indexing.make_batches(dFoF_smooth, num_batches=20)], axis=0)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#     np.save(f'/n/data1/hms/neurobio/sabatini/rich/spike_prob_{iter_day}.npy', np.single(spike_prob))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rich/Home_Linux_partition/github_repos/Cascade/cascade2p/cascade.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model_name, traces, model_folder, threshold, padding, verbosity)\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# Load config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The configuration file \"config.yaml\" can not be found at the location \"/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models/Global_EXC_30Hz_smoothing50ms_causalkernel/config.yaml\".\nYou have provided the model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the absolute or relative path \"/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models\".\nPlease check if there is a folder for model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the location \"/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models\"."
     ]
    }
   ],
   "source": [
    "spike_prob_all = []\n",
    "for iter_day,dir_S2p in enumerate(dir_batch):\n",
    "\n",
    "    F , Fneu , iscell , ops , spks , stat , num_frames_S2p = import_data.import_S2p(dir_S2p)\n",
    "\n",
    "    channelOffset_correction = 150\n",
    "    percentile_baseline = 10\n",
    "\n",
    "    dFoF , dF , F_neuSub , F_baseline = ca2p_preprocessing.make_dFoF(   F=F + channelOffset_correction,\n",
    "                                                                        Fneu=Fneu + channelOffset_correction,\n",
    "                                                                        neuropil_fraction=0.7,\n",
    "                                                                        percentile_baseline=percentile_baseline,\n",
    "                                                                        multicore_pref=True,\n",
    "                                                                        verbose=True)\n",
    "\n",
    "    dFoF_smooth = timeSeries.convolve_along_axis(dFoF,\n",
    "                                                kernel=math_functions.gaussian(np.arange(-15,15), 0, sig=50/30, plot_pref=False)[0],\n",
    "                                                axis=1,\n",
    "                                                mode='same',\n",
    "                                                multicore_pref=True,\n",
    "                                                verbose=True)\n",
    "\n",
    "    spike_prob = np.concatenate([cascade.predict(model_name='Global_EXC_30Hz_smoothing50ms_causalkernel',\n",
    "                                                 traces=batch, \n",
    "#                                                  model_folder='/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models', \n",
    "                                                 padding=0\n",
    "                                                ) for batch in indexing.make_batches(dFoF_smooth, num_batches=20)], axis=0)\n",
    "\n",
    "#     np.save(f'/n/data1/hms/neurobio/sabatini/rich/spike_prob_{iter_day}.npy', np.single(spike_prob))\n",
    "    spike_prob_all.append(np.single(spike_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c87c29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cascade2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cascade import P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14fd675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd3f9b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now check the updated available_models.yaml file for valid model names.\n",
      "File location: /media/rich/Home_Linux_partition/github_repos/Cascade/Pretrained_models/available_models.yaml\n"
     ]
    }
   ],
   "source": [
    "cascade.download_model('update_models', model_folder=str(Path(dir_github) / 'Cascade' / 'Pretrained_models'),verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37ec837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " List of available models: \n",
      "\n",
      "Global_EXC_30Hz_smoothing50ms_asymmetric_window_1_frame\n",
      "Global_EXC_30Hz_smoothing50ms_asymmetric_window_2_frames\n",
      "Global_EXC_30Hz_smoothing50ms_asymmetric_window_4_frames\n",
      "Global_EXC_30Hz_smoothing50ms_asymmetric_window_6_frames\n",
      "Global_EXC_30Hz_smoothing50ms_asymmetric_window_8_frames\n",
      "GCaMP6f_mouse_30Hz_smoothing200ms\n",
      "OGB_zf_pDp_7.5Hz_smoothing200ms\n",
      "OGB_zf_pDp_7.5Hz_smoothing200ms_causalkernel\n",
      "Global_EXC_3Hz_smoothing400ms\n",
      "Global_EXC_3Hz_smoothing400ms_causalkernel\n",
      "Global_EXC_4.25Hz_smoothing300ms\n",
      "Global_EXC_4.25Hz_smoothing300ms_causalkernel\n",
      "Global_EXC_5Hz_smoothing200ms\n",
      "Global_EXC_5Hz_smoothing200ms_causalkernel\n",
      "Global_EXC_6Hz_smoothing200ms\n",
      "Global_EXC_6Hz_smoothing200ms_causalkernel\n",
      "Global_EXC_7.5Hz_smoothing200ms\n",
      "Global_EXC_7.5Hz_smoothing200ms_causalkernel\n",
      "Global_EXC_10Hz_smoothing100ms\n",
      "Global_EXC_10Hz_smoothing100ms_causalkernel\n",
      "Global_EXC_10Hz_smoothing200ms\n",
      "Global_EXC_10Hz_smoothing200ms_causalkernel\n",
      "Global_EXC_12.5Hz_smoothing100ms\n",
      "Global_EXC_12.5Hz_smoothing100ms_causalkernel\n",
      "Global_EXC_12.5Hz_smoothing200ms\n",
      "Global_EXC_12.5Hz_smoothing200ms_causalkernel\n",
      "Global_EXC_15Hz_smoothing100ms\n",
      "Global_EXC_15Hz_smoothing100ms_causalkernel\n",
      "Global_EXC_15Hz_smoothing200ms\n",
      "Global_EXC_15Hz_smoothing200ms_causalkernel\n",
      "Global_EXC_17.5Hz_smoothing100ms\n",
      "Global_EXC_17.5Hz_smoothing200ms\n",
      "Global_EXC_17.5Hz_smoothing200ms_causalkernel\n",
      "Global_EXC_20Hz_smoothing100ms\n",
      "Global_EXC_20Hz_smoothing100ms_causalkernel\n",
      "Global_EXC_20Hz_smoothing200ms\n",
      "Global_EXC_20Hz_smoothing200ms_causalkernel\n",
      "Global_EXC_25Hz_smoothing100ms\n",
      "Global_EXC_25Hz_smoothing100ms_causalkernel\n",
      "Global_EXC_25Hz_smoothing50ms\n",
      "Global_EXC_25Hz_smoothing50ms_causalkernel\n",
      "Global_EXC_30Hz_smoothing100ms\n",
      "Global_EXC_30Hz_smoothing100ms_causalkernel\n",
      "Global_EXC_30Hz_smoothing200ms\n",
      "Global_EXC_30Hz_smoothing50ms\n",
      "Global_EXC_30Hz_smoothing50ms_causalkernel\n",
      "Global_INH_15Hz_smoothing100ms\n",
      "Global_INH_30Hz_smoothing50ms\n",
      "Global_INH_30Hz_smoothing100ms\n"
     ]
    }
   ],
   "source": [
    "yaml_file = open(str(Path(dir_github) / 'Cascade' / 'Pretrained_models' / 'available_models.yaml'))\n",
    "X = yaml.load(yaml_file, Loader=yaml.Loader)\n",
    "list_of_models = list(X.keys())\n",
    "print('\\n List of available models: \\n')\n",
    "for model in list_of_models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "955ae0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The configuration file \"config.yaml\" can not be found at the location \"/media/rich/Home_Linux_partition/github_repos/NBAP/Pretrained_models/Global_EXC_30Hz_smoothing50ms_causalkernel/config.yaml\".\n",
      "You have provided the model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the absolute or relative path \"Pretrained_models\".\n",
      "Please check if there is a folder for model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the location \"/media/rich/Home_Linux_partition/github_repos/NBAP/Pretrained_models\".\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The configuration file \"config.yaml\" can not be found at the location \"/media/rich/Home_Linux_partition/github_repos/NBAP/Pretrained_models/Global_EXC_30Hz_smoothing50ms_causalkernel/config.yaml\".\nYou have provided the model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the absolute or relative path \"Pretrained_models\".\nPlease check if there is a folder for model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the location \"/media/rich/Home_Linux_partition/github_repos/NBAP/Pretrained_models\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9957/150416506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#                                                  model_folder='/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                              \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                             ) for batch in indexing.make_batches(dFoF_smooth, num_batches=20)], axis=0)\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_9957/150416506.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#                                                  model_folder='/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                              \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                             ) for batch in indexing.make_batches(dFoF_smooth, num_batches=20)], axis=0)\n\u001b[0m",
      "\u001b[0;32m/media/rich/Home_Linux_partition/github_repos/Cascade/cascade2p/cascade.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model_name, traces, model_folder, threshold, padding, verbosity)\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# Load config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The configuration file \"config.yaml\" can not be found at the location \"/media/rich/Home_Linux_partition/github_repos/NBAP/Pretrained_models/Global_EXC_30Hz_smoothing50ms_causalkernel/config.yaml\".\nYou have provided the model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the absolute or relative path \"Pretrained_models\".\nPlease check if there is a folder for model \"Global_EXC_30Hz_smoothing50ms_causalkernel\" at the location \"/media/rich/Home_Linux_partition/github_repos/NBAP/Pretrained_models\"."
     ]
    }
   ],
   "source": [
    "spike_prob = np.concatenate([cascade.predict(model_name='Global_EXC_30Hz_smoothing50ms_causalkernel',\n",
    "                                             traces=batch, \n",
    "#                                                  model_folder='/n/data1/hms/neurobio/sabatini/rich/github_repos/Cascade/Pretrained_models', \n",
    "                                             padding=0\n",
    "                                            ) for batch in indexing.make_batches(dFoF_smooth, num_batches=20)], axis=0)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95ced2c4",
   "metadata": {},
   "source": [
    "np.save(f'/n/data1/hms/neurobio/sabatini/rich/spike_prob_all.npy', spike_prob_all)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dc60bf3",
   "metadata": {},
   "source": [
    "spike_prob = np.load('/n/data1/hms/neurobio/sabatini/rich/spike_prob.npy')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e92c446",
   "metadata": {},
   "source": [
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.plot(dFoF_smooth[3].T)\n",
    "plt.plot(spike_prob[3].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924c0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c1d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d06632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc4ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a856d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad3379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
